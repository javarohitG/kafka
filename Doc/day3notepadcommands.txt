kafka-topics.bat --alter --topic emp-topic --partitions 4 --zookeeper localhost:2181

Developer=0
Accountant=1
Architect=2



package com.jpmc.training.partitioner;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.Map;
import java.util.Properties;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

import com.jpmc.training.domain.Employee;

public class EmployeePartitioner implements Partitioner{
	
	private Properties props=new Properties();

	@Override
	public void configure(Map<String, ?> arg0) {
		// TODO Auto-generated method stub
		try {
			FileInputStream fin=new FileInputStream("designation.properties");
			props.load(fin);
			fin.close();
		} catch (FileNotFoundException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		
	}

	@Override
	public void close() {
		// TODO Auto-generated method stub
		props=null;
	}

	@Override
	public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
		// TODO Auto-generated method stub
		int partition=3;
		Employee employee=(Employee)value;
		String designation=employee.getDesignation();
		if(props.containsKey(designation)) {
			partition=Integer.parseInt(props.getProperty(designation));
		}
		return partition;
	}

}




package com.jpmc.training.sender;

import java.util.Properties;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

import com.jpmc.training.domain.Employee;
import com.jpmc.training.partitioner.EmployeePartitioner;
import com.jpmc.training.serializer.EmployeeSerializer;

public class EmployeeSenderWithCustomPartitioner {
public static void main(String[] args) {
	Properties props=new Properties();
	props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
	props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
	props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, EmployeeSerializer.class.getName());
	props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, EmployeePartitioner.class.getName());
	
	KafkaProducer<String,Employee> producer=new KafkaProducer<>(props);
	String topic="emp-topic";
	
	for(int i=1001;i<=1010;i++) {
	
	ProducerRecord<String, Employee> record=new ProducerRecord<String, Employee>(topic, 
			new Employee(i, "Name "+i, "Developer"));
	producer.send(record);
	}
	
	for(int i=2001;i<=2010;i++) {
		
		ProducerRecord<String, Employee> record=new ProducerRecord<String, Employee>(topic, 
				new Employee(i, "Name "+i, "Accountant"));
		producer.send(record);
		}
	
	for(int i=3001;i<=3010;i++) {
		
		ProducerRecord<String, Employee> record=new ProducerRecord<String, Employee>(topic, 
				new Employee(i, "Name "+i, "Architect"));
		producer.send(record);
		}
	for(int i=4001;i<=4010;i++) {
		
		ProducerRecord<String, Employee> record=new ProducerRecord<String, Employee>(topic, 
				new Employee(i, "Name "+i, "System Admin"));
		producer.send(record);
		}
	for(int i=5001;i<=5010;i++) {
		
		ProducerRecord<String, Employee> record=new ProducerRecord<String, Employee>(topic, 
				new Employee(i, "Name "+i, "Project Manager"));
		producer.send(record);
		}
	System.out.println("messages sent");
	producer.close();
}
}


package com.jpmc.training.receiver;

import java.time.Duration;
import java.util.Collection;
import java.util.Collections;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import com.jpmc.training.deserializer.EmployeeDeserializer;
import com.jpmc.training.domain.Employee;

public class EmployeeReceiverFromSpecificPartition {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, EmployeeDeserializer.class.getName());
		//props.put(ConsumerConfig.GROUP_ID_CONFIG,"group-2");
		
		KafkaConsumer<String, Employee> consumer=new KafkaConsumer<>(props);
		
		int partitionNo=Integer.parseInt(args[0]);
		TopicPartition partition=new TopicPartition("emp-topic", partitionNo);
		List<TopicPartition> partitions=Collections.singletonList(partition);
		
		consumer.assign(partitions);
		
		System.out.println("Waiting for messages from partition "+partitionNo);
		
		while(true) {
			ConsumerRecords<String, Employee> records=consumer.poll(Duration.ofSeconds(20));
			records.forEach(record->{
				System.out.println("key: "+record.key()+"\tpartition: "+record.partition());
				System.out.println("Value");
				Employee e=record.value();
				System.out.println(e.getEmpId()+"\t"+e.getName()+"\t"+e.getDesignation());
			});
		}
		
		

	}

}




package com.jpmc.training.receiver;

import java.time.Duration;
import java.util.Collection;
import java.util.Collections;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

public class SimpleReceiverWithOffsetInfo {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.GROUP_ID_CONFIG,"group-1");
		
		KafkaConsumer<String, String> consumer=new KafkaConsumer<>(props);
		
		Collection<String> topics=Collections.singletonList("first-topic");
		
		consumer.subscribe(topics);
		
		while(true) {
			ConsumerRecords<String, String> records=consumer.poll(Duration.ofSeconds(20));
			records.forEach(record->{
				System.out.println("key: "+record.key()+"\tvalue: "+record.value()+
						"\tpartition: "+record.partition()+"\toffset: "+record.offset());
			});
		}
		
		

	}

}




package com.jpmc.training.sender;

import java.util.Properties;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.serialization.StringSerializer;

public class SimpleWithCallback {
public static void main(String[] args) {
	Properties props=new Properties();
	props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
	props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
	props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
	
	KafkaProducer<String,String> producer=new KafkaProducer<>(props);
	String topic="first-topic";
	for(int i=1;i<=10;i++) {
		//Each message is represented as a ProducerRecord
		String key="first-key";
		ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, 
				key,"This is a test message "+i);
		producer.send(record,new MyCallaback(key));
	}
	for(int i=11;i<=20;i++) {
		String key="second-key";
		ProducerRecord<String, String> record=new ProducerRecord<String, String>(topic, 
				key,"This is a test message "+i);
		producer.send(record,new MyCallaback(key));
	}
	
	
	System.out.println("messages sent");
	producer.close();
}
}


class MyCallaback implements Callback{

	private String key;
	
	
	public MyCallaback(String key) {
		super();
		this.key = key;
	}


	@Override
	public void onCompletion(RecordMetadata rmd, Exception ex) {
		// TODO Auto-generated method stub
		if(ex==null) {
			System.out.println("message with key "+key+" delivered to partition "+rmd.partition()+
					" at offset "+rmd.offset());
		}
		
	}
	
}



props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");







package com.jpmc.training.receiver;

import java.time.Duration;
import java.util.Collection;
import java.util.Collections;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

public class SimpleReceiverWithManualCommit {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.GROUP_ID_CONFIG,"group-1");
		props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
		
		KafkaConsumer<String, String> consumer=new KafkaConsumer<>(props);
		
		Collection<String> topics=Collections.singletonList("first-topic");
		
		consumer.subscribe(topics);
		
		while(true) {
			ConsumerRecords<String, String> records=consumer.poll(Duration.ofSeconds(20));
			records.forEach(record->{
				System.out.println("key: "+record.key()+"\tvalue: "+record.value()+"\tpartition: "+record.partition()+
						"\t Offset: "+record.offset());
			});
			consumer.commitSync();
		}
		
		

	}

}




package com.jpmc.training.receiver;

import java.time.Duration;
import java.util.Collection;
import java.util.Collections;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import com.jpmc.training.deserializer.EmployeeDeserializer;
import com.jpmc.training.domain.Employee;

public class EmployeeReceiverFromSpecificPartitionAndOffset {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, EmployeeDeserializer.class.getName());
		//props.put(ConsumerConfig.GROUP_ID_CONFIG,"group-2");
		
		KafkaConsumer<String, Employee> consumer=new KafkaConsumer<>(props);
		
		int partitionNo=Integer.parseInt(args[0]);
		TopicPartition partition=new TopicPartition("emp-topic", partitionNo);
		List<TopicPartition> partitions=Collections.singletonList(partition);
		
		consumer.assign(partitions);
		
		System.out.println("Waiting for messages from partition "+partitionNo);
		
		while(true) {
			ConsumerRecords<String, Employee> records=consumer.poll(Duration.ofSeconds(20));
			records.forEach(record->{
				System.out.println("key: "+record.key()+"\tpartition: "+record.partition()+"\toffset: "+record.offset());
				System.out.println("Value");
				Employee e=record.value();
				System.out.println(e.getEmpId()+"\t"+e.getName()+"\t"+e.getDesignation());
			});
		}
		
		

	}

}





package com.jpmc.training.receiver;

import java.time.Duration;
import java.util.Collection;
import java.util.Collections;
import java.util.List;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

import com.jpmc.training.deserializer.EmployeeDeserializer;
import com.jpmc.training.domain.Employee;

public class EmployeeReceiverFromSpecificPartitionAndOffset {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		Properties props=new Properties();
		props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
		props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, EmployeeDeserializer.class.getName());
		//props.put(ConsumerConfig.GROUP_ID_CONFIG,"group-2");
		
		KafkaConsumer<String, Employee> consumer=new KafkaConsumer<>(props);
		
		int partitionNo=Integer.parseInt(args[0]);
		TopicPartition partition=new TopicPartition("emp-topic", partitionNo);
		List<TopicPartition> partitions=Collections.singletonList(partition);
		
		consumer.assign(partitions);
		int offset=Integer.parseInt(args[1]);
		consumer.seek(partition, offset);
		
		System.out.println("Waiting for messages from partition "+partitionNo);
		
		while(true) {
			ConsumerRecords<String, Employee> records=consumer.poll(Duration.ofSeconds(20));
			records.forEach(record->{
				System.out.println("key: "+record.key()+"\tpartition: "+record.partition()+"\toffset: "+record.offset());
				System.out.println("Value");
				Employee e=record.value();
				System.out.println(e.getEmpId()+"\t"+e.getName()+"\t"+e.getDesignation());
			});
		}
		
		

	}

}





steps :
1. create a copy of config\connect-file-source.properties and rename it as config\my-connect-file-source.properties

2. Modify the content of my-connect-file-source.properties as shown below.

name=local-file-source
connector.class=FileStreamSource
tasks.max=1
file=c:/input/test.txt
topic=connect-test-topic

3. create the connect-test-topic.

kafka-topics.bat --create --topic connect-test-topic --partitions 3 --replication-factor 1  --zookeeper localhost:2181

4. create a file called test.txt under c:\input directory and add some content to the file

5. start the kafka connect process.

open a new command window , navigate to c:\kafka_2.12-2.5.0 directory and type the following command.

bin\windows\connect-standalone.bat config\connect-standalone.properties config\my-connect-file-source.properties
(connect-process)			(connect-process config)		(connector-config)

kafka-console-consumer.bat --topic connect-test-topic --bootstrap-server localhost:9092 --from-beginning


